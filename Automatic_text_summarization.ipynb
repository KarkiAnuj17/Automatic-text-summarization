{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarkiAnuj17/Automatic-text-summarization/blob/main/Automatic_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUshmJtq0hCv",
        "outputId": "e935cf5d-f972-48dc-d93b-4f3865f7a005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EtxIWDza0kMp"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qo35kKPv0kRP"
      },
      "outputs": [],
      "source": [
        "import cudf\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oX8kZ610kT9",
        "outputId": "3c84600a-8b46-4d45-f6f9-e1be62271fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'article', 'highlights'], dtype='object')\n",
            "(11490, 3)\n"
          ]
        }
      ],
      "source": [
        "path=\"/content/dataset/dataset/test.csv\"\n",
        "df=pd.read_csv(path)\n",
        "print(df.columns)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wufX14cj0kWa",
        "outputId": "3a3ea884-e547-48f0-e58c-af083a8c76b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6lN5tja0kYz",
        "outputId": "812497ff-d31a-474d-8829-ca33a922bd6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11490/11490 [08:02<00:00, 23.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         id  \\\n",
            "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
            "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
            "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
            "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
            "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
            "\n",
            "                                             article  \\\n",
            "0  Ever noticed how plane seats appear to be gett...   \n",
            "1  A drunk teenage boy had to be rescued by secur...   \n",
            "2  Dougie Freedman is on the verge of agreeing a ...   \n",
            "3  Liverpool target Neto is also wanted by PSG an...   \n",
            "4  Bruce Jenner will break his silence in a two-h...   \n",
            "\n",
            "                                          highlights  \\\n",
            "0  Experts question if  packed out planes are put...   \n",
            "1  Drunk teenage boy climbed into lion enclosure ...   \n",
            "2  Nottingham Forest are close to extending Dougi...   \n",
            "3  Fiorentina goalkeeper Neto has been linked wit...   \n",
            "4  Tell-all interview with the reality TV star, 6...   \n",
            "\n",
            "                                      processed_text  \n",
            "0  [[ever, notice, plane, seat, appear, get, smal...  \n",
            "1  [[drunk, teenage, boy, rescue, security, jump,...  \n",
            "2  [[dougie, freedman, verge, agree, new, deal, r...  \n",
            "3  [[liverpool, target, neto, also, want, psg, cl...  \n",
            "4  [[bruce, jenner, break, silence, interview, di...  \n"
          ]
        }
      ],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    # Map POS tag to WordNet POS tag\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Split text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Tokenize words in the sentence\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Get POS tags\n",
        "        pos_tags = pos_tag(words)\n",
        "\n",
        "        # Lemmatize and remove stopwords\n",
        "        processed_words = []\n",
        "        for word, pos in pos_tags:\n",
        "            if word.lower() not in stop_words and word.isalnum():\n",
        "                lemmatized_word = lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
        "                processed_words.append(lemmatized_word)\n",
        "\n",
        "        # Append processed sentence\n",
        "        processed_sentences.append(processed_words)\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "def preprocess_csv(file_path, text_column):\n",
        "    # Read the CSV file into a cuDF DataFrame\n",
        "    df = cudf.read_csv(file_path)\n",
        "\n",
        "    # Convert text column to a pandas Series for processing with NLTK\n",
        "    text_series = df[text_column].to_pandas()\n",
        "\n",
        "    # Apply preprocessing to the text column with progress bar\n",
        "    processed_text = text_series.progress_apply(preprocess_text)\n",
        "\n",
        "    # Convert the processed text back to a cuDF DataFrame\n",
        "    df['processed_text'] = cudf.from_pandas(processed_text)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Enable the tqdm progress_apply\n",
        "tqdm.pandas()\n",
        "\n",
        "file_path = '/content/dataset/dataset/test.csv'\n",
        "text_column = 'article'\n",
        "processed_df = preprocess_csv(file_path, text_column)\n",
        "\n",
        "# Display the processed DataFrame\n",
        "print(processed_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E5-E7ZnMkum",
        "outputId": "2ad68323-6971-458f-ebe8-97f6a8c1d51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Articles: 100%|██████████| 11490/11490 [00:50<00:00, 226.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Cosine Similarity Matrix for the First Article:\n",
            " [[1.         0.03550455 0.         0.04097448 0.02367313 0.\n",
            "  0.         0.07210004 0.04264119 0.0403271  0.03092843 0.07493094\n",
            "  0.11954024 0.         0.02426954 0.06009226]\n",
            " [0.03550455 1.         0.         0.03528285 0.02038477 0.\n",
            "  0.08077285 0.02888779 0.03671804 0.         0.08621036 0.03002202\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         1.         0.53670032 0.0884582  0.12443423\n",
            "  0.         0.04675813 0.         0.         0.         0.\n",
            "  0.         0.         0.02943534 0.        ]\n",
            " [0.04097448 0.03528285 0.53670032 1.         0.0548335  0.04404127\n",
            "  0.         0.07770597 0.04237492 0.         0.0307353  0.0346473\n",
            "  0.         0.         0.02793048 0.        ]\n",
            " [0.02367313 0.02038477 0.0884582  0.0548335  1.         0.21782932\n",
            "  0.         0.04489486 0.07925059 0.         0.0574819  0.06479821\n",
            "  0.         0.         0.01613692 0.        ]\n",
            " [0.         0.         0.12443423 0.04404127 0.21782932 1.\n",
            "  0.         0.03605873 0.         0.         0.05588047 0.\n",
            "  0.         0.         0.02269982 0.        ]\n",
            " [0.         0.08077285 0.         0.         0.         0.\n",
            "  1.         0.         0.12116003 0.         0.07036216 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.07210004 0.02888779 0.04675813 0.07770597 0.04489486 0.03605873\n",
            "  0.         1.         0.03469441 0.03281158 0.02516449 0.06096652\n",
            "  0.09726226 0.         0.04261461 0.04889324]\n",
            " [0.04264119 0.03671804 0.         0.04237492 0.07925059 0.\n",
            "  0.12116003 0.03469441 1.         0.20961611 0.17509282 0.65027417\n",
            "  0.06625581 0.37012367 0.18428376 0.25783397]\n",
            " [0.0403271  0.         0.         0.         0.         0.\n",
            "  0.         0.03281158 0.20961611 1.         0.         0.20548975\n",
            "  0.10174    0.26724307 0.61528663 0.27861123]\n",
            " [0.03092843 0.08621036 0.         0.0307353  0.0574819  0.05588047\n",
            "  0.07036216 0.02516449 0.17509282 0.         1.         0.14316235\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.07493094 0.03002202 0.         0.0346473  0.06479821 0.\n",
            "  0.         0.06096652 0.65027417 0.20548975 0.14316235 1.\n",
            "  0.10108112 0.21769148 0.17119909 0.17992976]\n",
            " [0.11954024 0.         0.         0.         0.         0.\n",
            "  0.         0.09726226 0.06625581 0.10174    0.         0.10108112\n",
            "  1.         0.08447064 0.06122887 0.23285574]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.37012367 0.26724307 0.         0.21769148\n",
            "  0.08447064 1.         0.23494642 0.32871681]\n",
            " [0.02426954 0.         0.02943534 0.02793048 0.01613692 0.02269982\n",
            "  0.         0.04261461 0.18428376 0.61528663 0.         0.17119909\n",
            "  0.06122887 0.23494642 1.         0.34944244]\n",
            " [0.06009226 0.         0.         0.         0.         0.\n",
            "  0.         0.04889324 0.25783397 0.27861123 0.         0.17992976\n",
            "  0.23285574 0.32871681 0.34944244 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Cosine Similarity Function\n",
        "def cosine_similarity_matrix(matrix):\n",
        "    matrix = cp.array(matrix)\n",
        "    dot_product = cp.dot(matrix, matrix.T)  # Compute dot product\n",
        "    norm = cp.linalg.norm(matrix, axis=1)  # Compute norm (magnitude) of each vector\n",
        "    similarity_matrix = dot_product / (cp.outer(norm, norm) + 1e-10)  # Cosine similarity\n",
        "    return similarity_matrix\n",
        "\n",
        "# Vectorization Function using TF-IDF\n",
        "def vectorize_sentences_with_tfidf(sentences):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')  # Optional: Remove stopwords\n",
        "    term_doc_matrix_cpu = vectorizer.fit_transform(sentences)  # Compute the TF-IDF matrix\n",
        "\n",
        "    # Convert the sparse matrix to a dense NumPy array\n",
        "    term_doc_matrix_cpu = term_doc_matrix_cpu.toarray()\n",
        "\n",
        "    # Convert to CuPy array for GPU acceleration\n",
        "    term_doc_matrix = cp.array(term_doc_matrix_cpu)\n",
        "\n",
        "    return term_doc_matrix\n",
        "\n",
        "# Function to Compute Similarity for Articles\n",
        "def compute_similarity_for_articles(df, text_column='processed_text'):\n",
        "    text_series = df[text_column].to_pandas()  # Get text data from DataFrame\n",
        "    similarity_matrices = []\n",
        "\n",
        "    # Iterate over each article's processed text\n",
        "    for index, processed_text in tqdm(text_series.items(), total=len(text_series), desc=\"Processing Articles\"):\n",
        "\n",
        "        # Ensure processed_text is not empty and is a list of sentences\n",
        "        if processed_text and isinstance(processed_text, list):\n",
        "            processed_sentences = [' '.join(sentence) for sentence in processed_text]  # Join words into sentences\n",
        "        else:\n",
        "            processed_sentences = []\n",
        "\n",
        "        # Skip if the processed_sentences list is empty\n",
        "        if not processed_sentences:\n",
        "            similarity_matrices.append(None)\n",
        "            continue\n",
        "\n",
        "        # Vectorize sentences using TF-IDF approach\n",
        "        term_doc_matrix = vectorize_sentences_with_tfidf(processed_sentences)\n",
        "\n",
        "        # Compute the cosine similarity matrix\n",
        "        similarity_matrix = cosine_similarity_matrix(term_doc_matrix)\n",
        "\n",
        "        # Convert to CPU (NumPy) array for easier handling if necessary\n",
        "        similarity_matrix_cpu = cp.asnumpy(similarity_matrix)\n",
        "\n",
        "        # Store the similarity matrix for the article\n",
        "        similarity_matrices.append(similarity_matrix_cpu)\n",
        "\n",
        "    return similarity_matrices\n",
        "\n",
        "# Enable tqdm for progress bar in pandas apply\n",
        "tqdm.pandas()\n",
        "\n",
        "# Assuming you have a DataFrame `processed_df` with a 'processed_text' column\n",
        "final_similarity_matrices = compute_similarity_for_articles(processed_df, text_column='processed_text')\n",
        "\n",
        "# Example output for the first article\n",
        "print(\"Final Cosine Similarity Matrix for the First Article:\\n\", final_similarity_matrices[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d70tgfAaxqxi",
        "outputId": "7064e2bb-2034-4c23-cc1c-763939224863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Clusters for the First Document:\n",
            "[2 1 0 0 0 0 4 2 3 3 1 3 2 3 3 3]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "# Function to cluster sentences within a document\n",
        "def cluster_sentences(similarity_matrix, n_clusters=5):\n",
        "    # Ensure the similarity matrix is NumPy array\n",
        "    similarity_matrix = np.array(similarity_matrix)\n",
        "\n",
        "    # Convert similarity matrix to distance matrix (1 - similarity)\n",
        "    distance_matrix = 1 - similarity_matrix\n",
        "\n",
        "    # Ensure that the number of clusters does not exceed the number of sentences\n",
        "    n_clusters = min(n_clusters, distance_matrix.shape[0])\n",
        "\n",
        "    # Apply Agglomerative Clustering\n",
        "    clustering_model = AgglomerativeClustering(\n",
        "        n_clusters=n_clusters, metric='precomputed', linkage='average'\n",
        "    )\n",
        "    cluster_labels = clustering_model.fit_predict(distance_matrix)\n",
        "\n",
        "    return cluster_labels\n",
        "\n",
        "# Function to cluster sentences for all documents\n",
        "def cluster_sentences_for_documents(similarity_matrices, n_clusters=5):\n",
        "    sentence_clusters = []\n",
        "\n",
        "    for similarity_matrix in similarity_matrices:\n",
        "        if similarity_matrix is None:\n",
        "            sentence_clusters.append(None)  # Skip if no similarity matrix\n",
        "            continue\n",
        "\n",
        "        # Cluster sentences for the document\n",
        "        clusters = cluster_sentences(similarity_matrix, n_clusters)\n",
        "        sentence_clusters.append(clusters)\n",
        "\n",
        "    return sentence_clusters\n",
        "\n",
        "# Apply clustering to your computed similarity matrices\n",
        "n_clusters = 5  # Number of clusters per document (adjust based on data)\n",
        "sentence_clusters = cluster_sentences_for_documents(final_similarity_matrices, n_clusters=n_clusters)\n",
        "\n",
        "# Example: Display sentence clusters for the first document\n",
        "print(\"Sentence Clusters for the First Document:\")\n",
        "print(sentence_clusters[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to generate a word graph from clustered sentences\n",
        "def generate_word_graph(clustered_sentences, processed_sentences, threshold=1):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Iterate through each unique cluster index\n",
        "    for cluster_idx in set(clustered_sentences):\n",
        "        # Get sentences belonging to the current cluster\n",
        "        sentences_in_cluster = [processed_sentences[idx] for idx in range(len(clustered_sentences)) if clustered_sentences[idx] == cluster_idx]\n",
        "\n",
        "        # Iterate through each sentence in the cluster\n",
        "        for sentence in sentences_in_cluster:\n",
        "            # Get the unique words in the sentence\n",
        "            words = set(sentence)\n",
        "\n",
        "            # Create edges between all pairs of words in the sentence (undirected graph)\n",
        "            for word1, word2 in itertools.combinations(words, 2):\n",
        "                # Add edge with weight (frequency of co-occurrence)\n",
        "                if G.has_edge(word1, word2):\n",
        "                    G[word1][word2]['weight'] += 1\n",
        "                else:\n",
        "                    G.add_edge(word1, word2, weight=1)\n",
        "\n",
        "    # Remove edges with weight less than the threshold\n",
        "    edges_to_remove = [(u, v) for u, v, data in G.edges(data=True) if data['weight'] < threshold]\n",
        "    G.remove_edges_from(edges_to_remove)\n",
        "\n",
        "    return G\n",
        "\n",
        "# Function to process all articles without visualizing the word graph\n",
        "def process_all_articles(processed_df, sentence_clusters, threshold=1):\n",
        "    # Loop through all articles and process each article\n",
        "    for article_idx in tqdm(range(len(processed_df)), desc=\"Processing Articles\"):\n",
        "        # Get the clustered sentences and processed sentences for the current article\n",
        "        clustered_sentences = sentence_clusters[article_idx]\n",
        "        processed_sentences = processed_df['processed_text'].iloc[article_idx]\n",
        "\n",
        "        # Flatten the processed sentences if needed (flatten each sentence into individual words)\n",
        "        processed_sentences_flattened = [item for sublist in processed_sentences for item in sublist]\n",
        "\n",
        "        # Generate word graph for the clustered sentences\n",
        "        word_graph = generate_word_graph(clustered_sentences, processed_sentences_flattened, threshold)\n",
        "\n",
        "# Example: Process all articles without visualization\n",
        "process_all_articles(processed_df, sentence_clusters, threshold=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utPFQz72C0KL",
        "outputId": "5e13fab4-c786-4057-b088-63944de765dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Articles: 100%|██████████| 11490/11490 [00:57<00:00, 201.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6VwjGQaxxeD",
        "outputId": "29ab6a64-dc6c-4e7b-ce8a-77406a87f17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Articles: 100%|██████████| 11490/11490 [00:54<00:00, 209.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary for the First Article:\n",
            "ever notice plane seat appear get small small increase number people take sky expert question pack plane put passenger risk say shrink space aeroplane uncomfortable put health safety danger squabbling arm rest shrink space plane put health safety danger week consumer advisory group set department transportation say public hearing government happy set standard animal fly plane stipulate minimum amount space human\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "\n",
        "# Function to generate a word graph from clustered sentences\n",
        "def generate_word_graph(clustered_sentences, processed_sentences, threshold=1):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Iterate through each cluster in clustered_sentences\n",
        "    for cluster_idx in clustered_sentences:\n",
        "        # Ensure each cluster is a list of sentences\n",
        "        if isinstance(cluster_idx, list):\n",
        "            for sentence in cluster_idx:\n",
        "                # Ensure the sentence is a list of words\n",
        "                words = set(sentence)\n",
        "\n",
        "                # Create edges between all pairs of words in the sentence (undirected graph)\n",
        "                for word1, word2 in itertools.combinations(words, 2):\n",
        "                    # Add edge with weight (frequency of co-occurrence)\n",
        "                    if G.has_edge(word1, word2):\n",
        "                        G[word1][word2]['weight'] += 1\n",
        "                    else:\n",
        "                        G.add_edge(word1, word2, weight=1)\n",
        "\n",
        "    # Remove edges with weight less than the threshold\n",
        "    edges_to_remove = [(u, v) for u, v, data in G.edges(data=True) if data['weight'] < threshold]\n",
        "    G.remove_edges_from(edges_to_remove)\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "# Function to get the central words based on degree centrality\n",
        "def get_central_words(word_graph, top_n=10):\n",
        "    centrality = nx.degree_centrality(word_graph)\n",
        "    sorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
        "    central_words = [word for word, _ in sorted_centrality[:top_n]]\n",
        "    return central_words\n",
        "\n",
        "\n",
        "# Function to rank sentences based on central word count\n",
        "def rank_sentences_by_central_words(clustered_sentences, processed_sentences, central_words):\n",
        "    sentence_scores = []\n",
        "\n",
        "    for i, sentence in enumerate(processed_sentences):\n",
        "        central_word_count = sum(1 for word in sentence if word in central_words)\n",
        "        sentence_scores.append((i, central_word_count))\n",
        "\n",
        "    # Sort sentences by their centrality (most central words first)\n",
        "    ranked_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "    return ranked_sentences\n",
        "\n",
        "\n",
        "# Function to generate summary based on ranked sentences\n",
        "def generate_summary(ranked_sentences, processed_sentences, top_n_sentences=5):\n",
        "    summary_sentences = [processed_sentences[i] for i, _ in ranked_sentences[:top_n_sentences]]\n",
        "    summary = ' '.join([' '.join(sentence) for sentence in summary_sentences])\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Process all articles and generate summaries\n",
        "def process_all_articles(processed_df, sentence_clusters, top_n_sentences=5, word_graph_threshold=1, central_words_top_n=10):\n",
        "    all_summaries = []\n",
        "\n",
        "    # Use tqdm to add a progress bar to the iteration over the articles\n",
        "    for article_idx in tqdm(range(len(processed_df)), desc=\"Processing Articles\"):\n",
        "        # Get the clustered sentences for the current article\n",
        "        clustered_sentences = sentence_clusters[article_idx]\n",
        "\n",
        "        # Get the processed sentences for the current article\n",
        "        processed_sentences = processed_df['processed_text'].iloc[article_idx]\n",
        "\n",
        "        # Generate word graph for the clustered sentences\n",
        "        word_graph = generate_word_graph(clustered_sentences, processed_sentences, threshold=word_graph_threshold)\n",
        "\n",
        "        # Get the central words from the word graph\n",
        "        central_words = get_central_words(word_graph, top_n=central_words_top_n)\n",
        "\n",
        "        # Rank sentences by their central word count\n",
        "        ranked_sentences = rank_sentences_by_central_words(clustered_sentences, processed_sentences, central_words)\n",
        "\n",
        "        # Generate the summary for the current article\n",
        "        summary = generate_summary(ranked_sentences, processed_sentences, top_n_sentences)\n",
        "\n",
        "        all_summaries.append(summary)\n",
        "\n",
        "    return all_summaries\n",
        "\n",
        "\n",
        "# Example: Generate summaries for all articles in the dataset\n",
        "all_summaries = process_all_articles(processed_df, sentence_clusters, top_n_sentences=5, word_graph_threshold=1, central_words_top_n=10)\n",
        "\n",
        "# Print the generated summary for the first article\n",
        "print(\"Generated Summary for the First Article:\")\n",
        "print(all_summaries[0])  # Only print the summary for the first article\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Function to reconstruct and improve sentence formatting\n",
        "def reconstruct_summary(ranked_sentences, processed_sentences, top_n_sentences=5):\n",
        "    # Check the number of available ranked sentences and limit the selection to that number\n",
        "    max_sentences = min(top_n_sentences, len(ranked_sentences))\n",
        "\n",
        "    # Ensure we don't exceed the length of processed_sentences\n",
        "    summary_sentences = []\n",
        "    for i, _ in ranked_sentences[:max_sentences]:\n",
        "        if i < len(processed_sentences):  # Make sure the index is within range\n",
        "            summary_sentences.append(processed_sentences[i])\n",
        "\n",
        "    # Ensure each sentence is well-formed: capitalize and add a period if necessary\n",
        "    summary = []\n",
        "    for sentence in summary_sentences:\n",
        "        sentence = ' '.join(sentence)  # Join the words in the sentence\n",
        "        sentence = sentence.capitalize()  # Capitalize the first letter\n",
        "        if not sentence.endswith('.'):  # Add period at the end if missing\n",
        "            sentence += '.'\n",
        "        summary.append(sentence)\n",
        "\n",
        "    # Join the summary sentences into one final summary text\n",
        "    return ' '.join(summary)\n",
        "\n",
        "# Function to process all articles in the dataset\n",
        "def process_all_articles(processed_df, sentence_clusters, top_n_sentences=5, word_graph_threshold=1, central_words_top_n=10):\n",
        "    all_summaries = []\n",
        "\n",
        "    for article_idx in tqdm(range(len(processed_df))):  # Use tqdm to display progress for all articles\n",
        "        # Extract clustered sentences and processed sentences for the current article\n",
        "        clustered_sentences = sentence_clusters[article_idx]\n",
        "        processed_sentences = processed_df['processed_text'].iloc[article_idx]\n",
        "\n",
        "        # Flatten the processed sentences\n",
        "        processed_sentences_flattened = [item for sublist in processed_sentences for item in sublist]\n",
        "\n",
        "        # Generate word graph for the clustered sentences\n",
        "        word_graph = generate_word_graph(clustered_sentences, processed_sentences_flattened, threshold=word_graph_threshold)\n",
        "\n",
        "        # Get central words from the word graph\n",
        "        central_words = get_central_words(word_graph, top_n=central_words_top_n)\n",
        "\n",
        "        # Rank sentences based on central words\n",
        "        ranked_sentences = rank_sentences_by_central_words(clustered_sentences, processed_sentences_flattened, central_words)\n",
        "\n",
        "        # Reconstruct summary for the current article\n",
        "        summary = reconstruct_summary(ranked_sentences, processed_sentences, top_n_sentences)\n",
        "        all_summaries.append(summary)\n",
        "\n",
        "    return all_summaries\n",
        "\n",
        "# Example: Process all articles in the dataset and get the summary for the first article\n",
        "all_summaries = process_all_articles(processed_df, sentence_clusters, top_n_sentences=5, word_graph_threshold=1, central_words_top_n=10)\n",
        "\n",
        "# Print the summary for the first article\n",
        "print(\"Generated Summary for the First Article:\")\n",
        "print(all_summaries[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCLMNARM6gSt",
        "outputId": "64fecbdd-05f1-463e-9b00-5572b30c715f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11490/11490 [00:55<00:00, 208.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary for the First Article:\n",
            "Ever notice plane seat appear get small small. Increase number people take sky expert question pack plane put passenger risk. Say shrink space aeroplane uncomfortable put health safety danger. Squabbling arm rest shrink space plane put health safety danger. Week consumer advisory group set department transportation say public hearing government happy set standard animal fly plane stipulate minimum amount space human.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGFeKTTJx08q",
        "outputId": "5f498faf-f4f9-4f30-e14d-9138ab4cd6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Waiting for headers] [Wai\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,190 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,625 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,523 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,225 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [35.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [53.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,738 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,446 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,514 kB]\n",
            "Fetched 24.2 MB in 3s (9,357 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "locales is already the newest version (2.35-0ubuntu3.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Generating locales (this might take a while)...\n",
            "  en_US.UTF-8... done\n",
            "Generation complete.\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e6e1b95e9c81cb95301544d1ab17527810c92d25e94d244d09eb6880d2f0b315\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y locales\n",
        "!sudo locale-gen en_US.UTF-8\n",
        "!sudo update-locale LANG=en_US.UTF-8\n",
        "import locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91bSuIxXx4ZR",
        "outputId": "6bb9f687-1742-41df-a103-25a10571bd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11490/11490 [01:41<00:00, 112.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean ROUGE Scores across all articles:\n",
            "ROUGE-1: Precision: 0.2621, Recall: 0.3365, F-Measure: 0.2840\n",
            "ROUGE-2: Precision: 0.0774, Recall: 0.1014, F-Measure: 0.0845\n",
            "ROUGE-L: Precision: 0.1820, Recall: 0.2360, F-Measure: 0.1980\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Function to calculate ROUGE scores\n",
        "def calculate_rouge_score(generated_summary, reference_summary):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "    return scores\n",
        "\n",
        "# Function to calculate the mean ROUGE scores across all articles\n",
        "def calculate_mean_rouge_scores(all_rouge_scores):\n",
        "    # Initialize arrays to store scores for ROUGE-1, ROUGE-2, ROUGE-L\n",
        "    rouge1_precision = []\n",
        "    rouge1_recall = []\n",
        "    rouge1_fmeasure = []\n",
        "\n",
        "    rouge2_precision = []\n",
        "    rouge2_recall = []\n",
        "    rouge2_fmeasure = []\n",
        "\n",
        "    rougel_precision = []\n",
        "    rougel_recall = []\n",
        "    rougel_fmeasure = []\n",
        "\n",
        "    # Loop through the ROUGE scores for each article and extract individual metrics\n",
        "    for rouge_scores in all_rouge_scores:\n",
        "        rouge1_precision.append(rouge_scores['rouge1'].precision)\n",
        "        rouge1_recall.append(rouge_scores['rouge1'].recall)\n",
        "        rouge1_fmeasure.append(rouge_scores['rouge1'].fmeasure)\n",
        "\n",
        "        rouge2_precision.append(rouge_scores['rouge2'].precision)\n",
        "        rouge2_recall.append(rouge_scores['rouge2'].recall)\n",
        "        rouge2_fmeasure.append(rouge_scores['rouge2'].fmeasure)\n",
        "\n",
        "        rougel_precision.append(rouge_scores['rougeL'].precision)\n",
        "        rougel_recall.append(rouge_scores['rougeL'].recall)\n",
        "        rougel_fmeasure.append(rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "    # Calculate the mean of each metric\n",
        "    mean_rouge1_precision = np.mean(rouge1_precision)\n",
        "    mean_rouge1_recall = np.mean(rouge1_recall)\n",
        "    mean_rouge1_fmeasure = np.mean(rouge1_fmeasure)\n",
        "\n",
        "    mean_rouge2_precision = np.mean(rouge2_precision)\n",
        "    mean_rouge2_recall = np.mean(rouge2_recall)\n",
        "    mean_rouge2_fmeasure = np.mean(rouge2_fmeasure)\n",
        "\n",
        "    mean_rougel_precision = np.mean(rougel_precision)\n",
        "    mean_rougel_recall = np.mean(rougel_recall)\n",
        "    mean_rougel_fmeasure = np.mean(rougel_fmeasure)\n",
        "\n",
        "    # Return the mean scores\n",
        "    return {\n",
        "        'rouge1': {'precision': mean_rouge1_precision, 'recall': mean_rouge1_recall, 'fmeasure': mean_rouge1_fmeasure},\n",
        "        'rouge2': {'precision': mean_rouge2_precision, 'recall': mean_rouge2_recall, 'fmeasure': mean_rouge2_fmeasure},\n",
        "        'rougeL': {'precision': mean_rougel_precision, 'recall': mean_rougel_recall, 'fmeasure': mean_rougel_fmeasure}\n",
        "    }\n",
        "\n",
        "# Function to process all articles and calculate ROUGE scores\n",
        "def process_all_articles_and_evaluate(processed_df, sentence_clusters, highlights, top_n_sentences=5, word_graph_threshold=1, central_words_top_n=10):\n",
        "    all_summaries = []\n",
        "    all_rouge_scores = []\n",
        "\n",
        "    for article_idx in tqdm(range(len(processed_df))):  # Iterate over all articles\n",
        "        # Extract data for the current article\n",
        "        clustered_sentences = sentence_clusters[article_idx]\n",
        "        processed_sentences = processed_df['processed_text'].iloc[article_idx]\n",
        "\n",
        "        # Flatten the processed sentences\n",
        "        processed_sentences_flattened = [item for sublist in processed_sentences for item in sublist]\n",
        "\n",
        "        # Generate word graph for clustered sentences\n",
        "        word_graph = generate_word_graph(clustered_sentences, processed_sentences_flattened, threshold=word_graph_threshold)\n",
        "\n",
        "        # Get central words and rank sentences by those central words\n",
        "        central_words = get_central_words(word_graph, top_n=central_words_top_n)\n",
        "        ranked_sentences = rank_sentences_by_central_words(clustered_sentences, processed_sentences_flattened, central_words)\n",
        "\n",
        "        # Reconstruct summary for the current article\n",
        "        generated_summary = reconstruct_summary(ranked_sentences, processed_sentences, top_n_sentences)\n",
        "        all_summaries.append(generated_summary)\n",
        "\n",
        "        # Get the reference summary from the highlights of the article\n",
        "        reference_summary = highlights[article_idx]\n",
        "\n",
        "        # Calculate ROUGE scores between generated and reference summaries\n",
        "        rouge_scores = calculate_rouge_score(generated_summary, reference_summary)\n",
        "        all_rouge_scores.append(rouge_scores)\n",
        "\n",
        "    # Calculate mean ROUGE scores\n",
        "    mean_rouge_scores = calculate_mean_rouge_scores(all_rouge_scores)\n",
        "\n",
        "    return all_summaries, all_rouge_scores, mean_rouge_scores\n",
        "\n",
        "# Example: Process all articles and calculate ROUGE scores for each article\n",
        "# Assuming highlights is already a list of reference summaries (e.g., from processed_df['highlights'])\n",
        "highlights = processed_df['highlights'].to_arrow().to_pylist()  # Adjust according to your DataFrame structure\n",
        "all_summaries, all_rouge_scores, mean_rouge_scores = process_all_articles_and_evaluate(processed_df, sentence_clusters, highlights, top_n_sentences=5, word_graph_threshold=1, central_words_top_n=10)\n",
        "\n",
        "# Print mean ROUGE scores\n",
        "print(\"Mean ROUGE Scores across all articles:\")\n",
        "print(\"ROUGE-1: Precision: {:.4f}, Recall: {:.4f}, F-Measure: {:.4f}\".format(\n",
        "    mean_rouge_scores['rouge1']['precision'],\n",
        "    mean_rouge_scores['rouge1']['recall'],\n",
        "    mean_rouge_scores['rouge1']['fmeasure']\n",
        "))\n",
        "print(\"ROUGE-2: Precision: {:.4f}, Recall: {:.4f}, F-Measure: {:.4f}\".format(\n",
        "    mean_rouge_scores['rouge2']['precision'],\n",
        "    mean_rouge_scores['rouge2']['recall'],\n",
        "    mean_rouge_scores['rouge2']['fmeasure']\n",
        "))\n",
        "print(\"ROUGE-L: Precision: {:.4f}, Recall: {:.4f}, F-Measure: {:.4f}\".format(\n",
        "    mean_rouge_scores['rougeL']['precision'],\n",
        "    mean_rouge_scores['rougeL']['recall'],\n",
        "    mean_rouge_scores['rougeL']['fmeasure']\n",
        "))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}