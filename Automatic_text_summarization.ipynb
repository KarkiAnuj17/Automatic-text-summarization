{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarkiAnuj17/Automatic-text-summarization/blob/main/Automatic_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUshmJtq0hCv",
        "outputId": "09abce2e-927a-45c7-a6a2-763673c75dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EtxIWDza0kMp"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qo35kKPv0kRP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cudf\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oX8kZ610kT9",
        "outputId": "77edf859-5cf6-4ffd-8640-c429b669a045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'article', 'highlights'], dtype='object')\n",
            "(11490, 3)\n"
          ]
        }
      ],
      "source": [
        "path=\"/content/dataset/dataset/test.csv\"\n",
        "df=pd.read_csv(path)\n",
        "print(df.columns)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wufX14cj0kWa",
        "outputId": "40c8ab54-c621-4ee6-e9fe-5a4b07c5ffc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6lN5tja0kYz",
        "outputId": "fa002c03-ae22-4057-ed18-3e2f8241b2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11490/11490 [09:04<00:00, 21.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         id  \\\n",
            "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
            "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
            "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
            "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
            "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
            "\n",
            "                                             article  \\\n",
            "0  Ever noticed how plane seats appear to be gett...   \n",
            "1  A drunk teenage boy had to be rescued by secur...   \n",
            "2  Dougie Freedman is on the verge of agreeing a ...   \n",
            "3  Liverpool target Neto is also wanted by PSG an...   \n",
            "4  Bruce Jenner will break his silence in a two-h...   \n",
            "\n",
            "                                          highlights  \\\n",
            "0  Experts question if  packed out planes are put...   \n",
            "1  Drunk teenage boy climbed into lion enclosure ...   \n",
            "2  Nottingham Forest are close to extending Dougi...   \n",
            "3  Fiorentina goalkeeper Neto has been linked wit...   \n",
            "4  Tell-all interview with the reality TV star, 6...   \n",
            "\n",
            "                                      processed_text  \n",
            "0  [[ever, notice, plane, seat, appear, get, smal...  \n",
            "1  [[drunk, teenage, boy, rescue, security, jump,...  \n",
            "2  [[dougie, freedman, verge, agree, new, deal, r...  \n",
            "3  [[liverpool, target, neto, also, want, psg, cl...  \n",
            "4  [[bruce, jenner, break, silence, interview, di...  \n"
          ]
        }
      ],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    # Map POS tag to WordNet POS tag\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Split text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Tokenize words in the sentence\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Get POS tags\n",
        "        pos_tags = pos_tag(words)\n",
        "\n",
        "        # Lemmatize and remove stopwords\n",
        "        processed_words = []\n",
        "        for word, pos in pos_tags:\n",
        "            if word.lower() not in stop_words and word.isalnum():\n",
        "                lemmatized_word = lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
        "                processed_words.append(lemmatized_word)\n",
        "\n",
        "        # Append processed sentence\n",
        "        processed_sentences.append(processed_words)\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "def preprocess_csv(file_path, text_column):\n",
        "    # Read the CSV file into a cuDF DataFrame\n",
        "    df = cudf.read_csv(file_path)\n",
        "\n",
        "    # Convert text column to a pandas Series for processing with NLTK\n",
        "    text_series = df[text_column].to_pandas()\n",
        "\n",
        "    # Apply preprocessing to the text column with progress bar\n",
        "    processed_text = text_series.progress_apply(preprocess_text)\n",
        "\n",
        "    # Convert the processed text back to a cuDF DataFrame\n",
        "    df['processed_text'] = cudf.from_pandas(processed_text)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Enable the tqdm progress_apply\n",
        "tqdm.pandas()\n",
        "\n",
        "file_path = '/content/dataset/dataset/test.csv'\n",
        "text_column = 'article'\n",
        "processed_df = preprocess_csv(file_path, text_column)\n",
        "\n",
        "# Display the processed DataFrame\n",
        "print(processed_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGGkaA3FffZn",
        "outputId": "172aca5f-b154-4ca0-a762-338636067138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Articles: 100%|██████████| 11490/11490 [01:21<00:00, 141.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Cosine Similarity Matrix for the First Article:\n",
            " [[1.         0.09128709 0.         0.1        0.06454972 0.\n",
            "  0.         0.16329932 0.09534626 0.0766965  0.08451543 0.16903085\n",
            "  0.21764288 0.         0.048795   0.11952286]\n",
            " [0.09128709 1.         0.09622504 0.18257419 0.05892557 0.\n",
            "  0.20412415 0.0745356  0.08703883 0.         0.15430335 0.07715167\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.09622504 1.         0.63245553 0.13608276 0.19245009\n",
            "  0.         0.0860663  0.         0.         0.         0.\n",
            "  0.         0.         0.05143445 0.        ]\n",
            " [0.1        0.18257419 0.63245553 1.         0.12909944 0.09128709\n",
            "  0.         0.16329932 0.09534626 0.         0.08451543 0.08451543\n",
            "  0.         0.         0.048795   0.        ]\n",
            " [0.06454972 0.05892557 0.13608276 0.12909944 1.         0.29462783\n",
            "  0.         0.10540926 0.12309149 0.         0.10910895 0.10910895\n",
            "  0.         0.         0.03149704 0.        ]\n",
            " [0.         0.         0.19245009 0.09128709 0.29462783 1.\n",
            "  0.         0.0745356  0.         0.         0.07715167 0.\n",
            "  0.         0.         0.04454354 0.        ]\n",
            " [0.         0.20412415 0.         0.         0.         0.\n",
            "  1.         0.         0.10660036 0.         0.09449112 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.16329932 0.0745356  0.0860663  0.16329932 0.10540926 0.0745356\n",
            "  0.         1.         0.07784989 0.06262243 0.06900656 0.13801311\n",
            "  0.17770466 0.         0.07968191 0.09759001]\n",
            " [0.09534626 0.08703883 0.         0.09534626 0.12309149 0.\n",
            "  0.10660036 0.07784989 1.         0.29250897 0.24174689 0.72524067\n",
            "  0.06917145 0.49236596 0.27914526 0.34188173]\n",
            " [0.0766965  0.         0.         0.         0.         0.\n",
            "  0.         0.06262243 0.29250897 1.         0.         0.32410186\n",
            "  0.16692447 0.39605902 0.71105713 0.41251432]\n",
            " [0.08451543 0.15430335 0.         0.08451543 0.10910895 0.07715167\n",
            "  0.09449112 0.06900656 0.24174689 0.         1.         0.21428571\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.16903085 0.07715167 0.         0.08451543 0.10910895 0.\n",
            "  0.         0.13801311 0.72524067 0.32410186 0.21428571 1.\n",
            "  0.1839418  0.32732684 0.28867513 0.30304576]\n",
            " [0.21764288 0.         0.         0.         0.         0.\n",
            "  0.         0.17770466 0.06917145 0.16692447 0.         0.1839418\n",
            "  1.         0.09365858 0.10619885 0.34684399]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.49236596 0.39605902 0.         0.32732684\n",
            "  0.09365858 1.         0.37796447 0.46291005]\n",
            " [0.048795   0.         0.05143445 0.048795   0.03149704 0.04454354\n",
            "  0.         0.07968191 0.27914526 0.71105713 0.         0.28867513\n",
            "  0.10619885 0.37796447 1.         0.49573007]\n",
            " [0.11952286 0.         0.         0.         0.         0.\n",
            "  0.         0.09759001 0.34188173 0.41251432 0.         0.30304576\n",
            "  0.34684399 0.46291005 0.49573007 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "import cudf\n",
        "import cupy as cp\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "def compute_similarity_for_articles(df, text_column='processed_text'):\n",
        "    text_series = df[text_column].to_pandas()\n",
        "    similarity_matrices = []  # To store the similarity matrices for each article\n",
        "\n",
        "    # Iterate over each article\n",
        "    for index, processed_text in tqdm(text_series.items(), total=len(text_series), desc=\"Processing Articles\"):\n",
        "        processed_sentences = [' '.join(sentence) for sentence in processed_text]  # Ensure tokenized input\n",
        "\n",
        "        # Vectorize sentences\n",
        "        term_doc_matrix = vectorize_sentences(processed_text)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        similarity_matrix = cosine_similarity_matrix(term_doc_matrix)\n",
        "\n",
        "        # Convert to CPU if needed for output (optional if needed)\n",
        "        similarity_matrix_cpu = cp.asnumpy(similarity_matrix)\n",
        "\n",
        "        # Append the result to the list\n",
        "        similarity_matrices.append(similarity_matrix_cpu)\n",
        "\n",
        "    return similarity_matrices\n",
        "\n",
        "# Compute and store similarity matrices for each article\n",
        "final_similarity_matrices = compute_similarity_for_articles(processed_df, text_column='processed_text')\n",
        "\n",
        "# Display the final output for the first article\n",
        "print(\"Final Cosine Similarity Matrix for the First Article:\\n\", final_similarity_matrices[0])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}