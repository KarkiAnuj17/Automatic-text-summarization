{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarkiAnuj17/Automatic-text-summarization/blob/main/Automatic_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QhkQlHY6Weau"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OjaejzEtzqHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e9050e-c446-4927-b58f-dcce07b083c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "Ed5lZM36oH2C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/dataset/dataset/test.csv\"\n",
        "df=pd.read_csv(path)\n",
        "print(df.columns)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZil5Rj9rD34",
        "outputId": "eadc231c-4aef-4739-9df9-a8d02dc4368a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'article', 'highlights'], dtype='object')\n",
            "(11490, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "tqdm.pandas()\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "DH5iV3rMreUp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_doc(doc):\n",
        "    try:\n",
        "        sentences = [sent.text for sent in doc.sents]  # Sentence splitting\n",
        "        lemmatized_tokens = []\n",
        "        for sent in doc.sents:\n",
        "            tokens = [token.text for token in sent]  # Tokenization\n",
        "            tokens_no_stop = [token.lemma_ for token in sent if not token.is_stop]  # Remove stop words and lemmatize\n",
        "            lemmatized_tokens.append(tokens_no_stop)\n",
        "        return {'sentences': sentences, 'tokens': tokens, 'tokens_no_stop': lemmatized_tokens}\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing document: {e}\")\n",
        "        return {'sentences': [], 'tokens': [], 'tokens_no_stop': []}\n",
        "def process_docs_with_progress(docs):\n",
        "    processed_docs = []\n",
        "    for doc in tqdm(docs, desc='Processing Documents', total=len(df)):\n",
        "        processed_docs.append(process_single_doc(doc))\n",
        "    return processed_docs"
      ],
      "metadata": {
        "id": "-RnejTZUoLFl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use nlp.pipe to process documents in batches with reduced batch_size and n_process\n",
        "docs_generator = nlp.pipe(df['article'], batch_size=20, n_process=2)\n",
        "\n",
        "# Convert the generator to a list and process with progress bar\n",
        "processed_data = process_docs_with_progress(docs_generator)\n",
        "\n",
        "# Extract sentences, tokens, and tokens_no_stop from processed_data\n",
        "df['sentences'] = [item['sentences'] for item in processed_data]\n",
        "df['tokens'] = [item['tokens'] for item in processed_data]\n",
        "df['tokens_no_stop'] = [item['tokens_no_stop'] for item in processed_data]\n",
        "\n",
        "# Show processed data\n",
        "print(df[['article','tokens_no_stop']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrvRuMcNvDkE",
        "outputId": "a4119c9c-3533-4298-ebc9-62d4d576cc39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Documents: 100%|██████████| 11490/11490 [19:49<00:00,  9.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             article  \\\n",
            "0  Ever noticed how plane seats appear to be gett...   \n",
            "1  A drunk teenage boy had to be rescued by secur...   \n",
            "2  Dougie Freedman is on the verge of agreeing a ...   \n",
            "3  Liverpool target Neto is also wanted by PSG an...   \n",
            "4  Bruce Jenner will break his silence in a two-h...   \n",
            "\n",
            "                                           sentences  \\\n",
            "0  [Ever noticed how plane seats appear to be get...   \n",
            "1  [A drunk teenage boy had to be rescued by secu...   \n",
            "2  [Dougie Freedman is on the verge of agreeing a...   \n",
            "3  [Liverpool target Neto is also wanted by PSG a...   \n",
            "4  [Bruce Jenner will break his silence in a two-...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  [British, Airways, has, a, seat, pitch, of, 31...   \n",
            "1  [Last, year, a, 20, -, year, -, old, man, was,...   \n",
            "2  [That, has, not, prevented, Forest, 's, owners...   \n",
            "3  [Neto, joined, Fiorentina, from,  , Atletico, ...   \n",
            "4  [In, February, ,, she, presented, ', A, Nation...   \n",
            "\n",
            "                                      tokens_no_stop  \n",
            "0  [[notice, plane, seat, appear, get, small, sma...  \n",
            "1  [[drunk, teenage, boy, rescue, security, jump,...  \n",
            "2  [[Dougie, Freedman, verge, agree, new, -, year...  \n",
            "3  [[liverpool, target, Neto, want, PSG, club, Sp...  \n",
            "4  [[Bruce, Jenner, break, silence, -, hour, inte...  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1tjT8m0KTEIOENo8ueGMPKhbCLZ1IhCGZ",
      "authorship_tag": "ABX9TyOvneCvc4gxffAzAmkdPuTb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}