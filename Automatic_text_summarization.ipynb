{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarkiAnuj17/Automatic-text-summarization/blob/main/Automatic_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjaejzEtzqHC",
        "outputId": "b2a644cc-5231-4a75-e4c9-453e99c86104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed5lZM36oH2C"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz5MsFdbiU7u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cudf\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZil5Rj9rD34",
        "outputId": "10685b52-c5b7-49fa-b372-8b9b536039ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'article', 'highlights'], dtype='object')\n",
            "(11490, 3)\n"
          ]
        }
      ],
      "source": [
        "path=\"/content/dataset/dataset/test.csv\"\n",
        "df=pd.read_csv(path)\n",
        "print(df.columns)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH5iV3rMreUp",
        "outputId": "f0df25cd-38a2-4dd3-f001-977be14161e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RnejTZUoLFl",
        "outputId": "53be4560-2e6c-4a7c-f334-6589133bb983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11490/11490 [08:57<00:00, 21.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         id  \\\n",
            "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
            "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
            "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
            "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
            "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
            "\n",
            "                                             article  \\\n",
            "0  Ever noticed how plane seats appear to be gett...   \n",
            "1  A drunk teenage boy had to be rescued by secur...   \n",
            "2  Dougie Freedman is on the verge of agreeing a ...   \n",
            "3  Liverpool target Neto is also wanted by PSG an...   \n",
            "4  Bruce Jenner will break his silence in a two-h...   \n",
            "\n",
            "                                          highlights  \\\n",
            "0  Experts question if  packed out planes are put...   \n",
            "1  Drunk teenage boy climbed into lion enclosure ...   \n",
            "2  Nottingham Forest are close to extending Dougi...   \n",
            "3  Fiorentina goalkeeper Neto has been linked wit...   \n",
            "4  Tell-all interview with the reality TV star, 6...   \n",
            "\n",
            "                                      processed_text  \n",
            "0  [[ever, notice, plane, seat, appear, get, smal...  \n",
            "1  [[drunk, teenage, boy, rescue, security, jump,...  \n",
            "2  [[dougie, freedman, verge, agree, new, deal, r...  \n",
            "3  [[liverpool, target, neto, also, want, psg, cl...  \n",
            "4  [[bruce, jenner, break, silence, interview, di...  \n"
          ]
        }
      ],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    # Map POS tag to WordNet POS tag\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if not found\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Split text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Tokenize words in the sentence\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Get POS tags\n",
        "        pos_tags = pos_tag(words)\n",
        "\n",
        "        # Lemmatize and remove stopwords\n",
        "        processed_words = []\n",
        "        for word, pos in pos_tags:\n",
        "            if word.lower() not in stop_words and word.isalnum():\n",
        "                lemmatized_word = lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
        "                processed_words.append(lemmatized_word)\n",
        "\n",
        "        # Append processed sentence\n",
        "        processed_sentences.append(processed_words)\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "def preprocess_csv(file_path, text_column):\n",
        "    # Read the CSV file into a cuDF DataFrame\n",
        "    df = cudf.read_csv(file_path)\n",
        "\n",
        "    # Convert text column to a pandas Series for processing with NLTK\n",
        "    text_series = df[text_column].to_pandas()\n",
        "\n",
        "    # Apply preprocessing to the text column with progress bar\n",
        "    processed_text = text_series.progress_apply(preprocess_text)\n",
        "\n",
        "    # Convert the processed text back to a cuDF DataFrame\n",
        "    df['processed_text'] = cudf.from_pandas(processed_text)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Enable the tqdm progress_apply\n",
        "tqdm.pandas()\n",
        "\n",
        "file_path = '/content/dataset/dataset/test.csv'\n",
        "text_column = 'article'\n",
        "processed_df = preprocess_csv(file_path, text_column)\n",
        "\n",
        "# Display the processed DataFrame\n",
        "print(processed_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrvRuMcNvDkE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRonYQy_XCrT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOgyF4nLXG3y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1tjT8m0KTEIOENo8ueGMPKhbCLZ1IhCGZ",
      "authorship_tag": "ABX9TyMRGitEK2/1TZIX3/mvg2mT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}