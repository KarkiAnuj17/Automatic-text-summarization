{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarkiAnuj17/Automatic-text-summarization/blob/main/Automatic_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HUshmJtq0hCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f679556d-48c6-44b3-fb76-0cad34360d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EtxIWDza0kMp"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qo35kKPv0kRP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cudf\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_oX8kZ610kT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4becb93-42b9-4bc9-c440-9e3d19788665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'article', 'highlights'], dtype='object')\n",
            "(11490, 3)\n"
          ]
        }
      ],
      "source": [
        "path=\"/content/dataset/dataset/test.csv\"\n",
        "df=pd.read_csv(path)\n",
        "print(df.columns)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wufX14cj0kWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1b1004-7c9e-4cd3-dd36-d7b2783c6734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6lN5tja0kYz",
        "outputId": "1cbb719b-261b-45b2-bd43-5dc4bf8f55f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11490/11490 [07:53<00:00, 24.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         id  \\\n",
            "0  92c514c913c0bdfe25341af9fd72b29db544099b   \n",
            "1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n",
            "2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n",
            "3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n",
            "4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n",
            "\n",
            "                                             article  \\\n",
            "0  Ever noticed how plane seats appear to be gett...   \n",
            "1  A drunk teenage boy had to be rescued by secur...   \n",
            "2  Dougie Freedman is on the verge of agreeing a ...   \n",
            "3  Liverpool target Neto is also wanted by PSG an...   \n",
            "4  Bruce Jenner will break his silence in a two-h...   \n",
            "\n",
            "                                          highlights  \\\n",
            "0  Experts question if  packed out planes are put...   \n",
            "1  Drunk teenage boy climbed into lion enclosure ...   \n",
            "2  Nottingham Forest are close to extending Dougi...   \n",
            "3  Fiorentina goalkeeper Neto has been linked wit...   \n",
            "4  Tell-all interview with the reality TV star, 6...   \n",
            "\n",
            "                                      processed_text  \n",
            "0  [[ever, notice, plane, seat, appear, get, smal...  \n",
            "1  [[drunk, teenage, boy, rescue, security, jump,...  \n",
            "2  [[dougie, freedman, verge, agree, new, deal, r...  \n",
            "3  [[liverpool, target, neto, also, want, psg, cl...  \n",
            "4  [[bruce, jenner, break, silence, interview, di...  \n"
          ]
        }
      ],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    # Map POS tag to WordNet POS tag\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Split text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Tokenize words in the sentence\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Get POS tags\n",
        "        pos_tags = pos_tag(words)\n",
        "\n",
        "        # Lemmatize and remove stopwords\n",
        "        processed_words = []\n",
        "        for word, pos in pos_tags:\n",
        "            if word.lower() not in stop_words and word.isalnum():\n",
        "                lemmatized_word = lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos))\n",
        "                processed_words.append(lemmatized_word)\n",
        "\n",
        "        # Append processed sentence\n",
        "        processed_sentences.append(processed_words)\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "def preprocess_csv(file_path, text_column):\n",
        "    # Read the CSV file into a cuDF DataFrame\n",
        "    df = cudf.read_csv(file_path)\n",
        "\n",
        "    # Convert text column to a pandas Series for processing with NLTK\n",
        "    text_series = df[text_column].to_pandas()\n",
        "\n",
        "    # Apply preprocessing to the text column with progress bar\n",
        "    processed_text = text_series.progress_apply(preprocess_text)\n",
        "\n",
        "    # Convert the processed text back to a cuDF DataFrame\n",
        "    df['processed_text'] = cudf.from_pandas(processed_text)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Enable the tqdm progress_apply\n",
        "tqdm.pandas()\n",
        "\n",
        "file_path = '/content/dataset/dataset/test.csv'\n",
        "text_column = 'article'\n",
        "processed_df = preprocess_csv(file_path, text_column)\n",
        "\n",
        "# Display the processed DataFrame\n",
        "print(processed_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Cosine Similarity Function\n",
        "def cosine_similarity_matrix(matrix):\n",
        "    matrix = cp.array(matrix)\n",
        "    dot_product = cp.dot(matrix, matrix.T)  # Compute dot product\n",
        "    norm = cp.linalg.norm(matrix, axis=1)  # Compute norm (magnitude) of each vector\n",
        "    similarity_matrix = dot_product / (cp.outer(norm, norm) + 1e-10)  # Cosine similarity\n",
        "    return similarity_matrix\n",
        "\n",
        "# Vectorization Function using TF-IDF\n",
        "def vectorize_sentences_with_tfidf(sentences):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')  # Optional: Remove stopwords\n",
        "    term_doc_matrix_cpu = vectorizer.fit_transform(sentences)  # Compute the TF-IDF matrix\n",
        "\n",
        "    # Convert the sparse matrix to a dense NumPy array\n",
        "    term_doc_matrix_cpu = term_doc_matrix_cpu.toarray()\n",
        "\n",
        "    # Convert to CuPy array for GPU acceleration\n",
        "    term_doc_matrix = cp.array(term_doc_matrix_cpu)\n",
        "\n",
        "    return term_doc_matrix\n",
        "\n",
        "# Function to Compute Similarity for Articles\n",
        "def compute_similarity_for_articles(df, text_column='processed_text'):\n",
        "    text_series = df[text_column].to_pandas()  # Get text data from DataFrame\n",
        "    similarity_matrices = []\n",
        "\n",
        "    # Iterate over each article's processed text\n",
        "    for index, processed_text in tqdm(text_series.items(), total=len(text_series), desc=\"Processing Articles\"):\n",
        "        processed_sentences = [' '.join(sentence) for sentence in processed_text]  # Join words into sentences\n",
        "\n",
        "        # Vectorize sentences using TF-IDF approach\n",
        "        term_doc_matrix = vectorize_sentences_with_tfidf(processed_sentences)\n",
        "\n",
        "        # Compute the cosine similarity matrix\n",
        "        similarity_matrix = cosine_similarity_matrix(term_doc_matrix)\n",
        "\n",
        "        # Convert to CPU (NumPy) array for easier handling if necessary\n",
        "        similarity_matrix_cpu = cp.asnumpy(similarity_matrix)\n",
        "\n",
        "        # Store the similarity matrix for the article\n",
        "        similarity_matrices.append(similarity_matrix_cpu)\n",
        "\n",
        "    return similarity_matrices\n",
        "\n",
        "# Enable tqdm for progress bar in pandas apply\n",
        "tqdm.pandas()\n",
        "\n",
        "# Assuming you have a DataFrame `processed_df` with a 'processed_text' column\n",
        "final_similarity_matrices = compute_similarity_for_articles(processed_df, text_column='processed_text')\n",
        "\n",
        "# Example output for the first article\n",
        "print(\"Final Cosine Similarity Matrix for the First Article:\\n\", final_similarity_matrices[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcyWaBI8EbJt",
        "outputId": "e04fecd7-c6f3-4942-c276-98c9edc7432c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Articles: 100%|██████████| 11490/11490 [00:49<00:00, 234.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Cosine Similarity Matrix for the First Article:\n",
            " [[1.         0.03550455 0.         0.04097448 0.02367313 0.\n",
            "  0.         0.07210004 0.04264119 0.0403271  0.03092843 0.07493094\n",
            "  0.11954024 0.         0.02426954 0.06009226]\n",
            " [0.03550455 1.         0.         0.03528285 0.02038477 0.\n",
            "  0.08077285 0.02888779 0.03671804 0.         0.08621036 0.03002202\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         1.         0.53670032 0.0884582  0.12443423\n",
            "  0.         0.04675813 0.         0.         0.         0.\n",
            "  0.         0.         0.02943534 0.        ]\n",
            " [0.04097448 0.03528285 0.53670032 1.         0.0548335  0.04404127\n",
            "  0.         0.07770597 0.04237492 0.         0.0307353  0.0346473\n",
            "  0.         0.         0.02793048 0.        ]\n",
            " [0.02367313 0.02038477 0.0884582  0.0548335  1.         0.21782932\n",
            "  0.         0.04489486 0.07925059 0.         0.0574819  0.06479821\n",
            "  0.         0.         0.01613692 0.        ]\n",
            " [0.         0.         0.12443423 0.04404127 0.21782932 1.\n",
            "  0.         0.03605873 0.         0.         0.05588047 0.\n",
            "  0.         0.         0.02269982 0.        ]\n",
            " [0.         0.08077285 0.         0.         0.         0.\n",
            "  1.         0.         0.12116003 0.         0.07036216 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.07210004 0.02888779 0.04675813 0.07770597 0.04489486 0.03605873\n",
            "  0.         1.         0.03469441 0.03281158 0.02516449 0.06096652\n",
            "  0.09726226 0.         0.04261461 0.04889324]\n",
            " [0.04264119 0.03671804 0.         0.04237492 0.07925059 0.\n",
            "  0.12116003 0.03469441 1.         0.20961611 0.17509282 0.65027417\n",
            "  0.06625581 0.37012367 0.18428376 0.25783397]\n",
            " [0.0403271  0.         0.         0.         0.         0.\n",
            "  0.         0.03281158 0.20961611 1.         0.         0.20548975\n",
            "  0.10174    0.26724307 0.61528663 0.27861123]\n",
            " [0.03092843 0.08621036 0.         0.0307353  0.0574819  0.05588047\n",
            "  0.07036216 0.02516449 0.17509282 0.         1.         0.14316235\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.07493094 0.03002202 0.         0.0346473  0.06479821 0.\n",
            "  0.         0.06096652 0.65027417 0.20548975 0.14316235 1.\n",
            "  0.10108112 0.21769148 0.17119909 0.17992976]\n",
            " [0.11954024 0.         0.         0.         0.         0.\n",
            "  0.         0.09726226 0.06625581 0.10174    0.         0.10108112\n",
            "  1.         0.08447064 0.06122887 0.23285574]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.37012367 0.26724307 0.         0.21769148\n",
            "  0.08447064 1.         0.23494642 0.32871681]\n",
            " [0.02426954 0.         0.02943534 0.02793048 0.01613692 0.02269982\n",
            "  0.         0.04261461 0.18428376 0.61528663 0.         0.17119909\n",
            "  0.06122887 0.23494642 1.         0.34944244]\n",
            " [0.06009226 0.         0.         0.         0.         0.\n",
            "  0.         0.04889324 0.25783397 0.27861123 0.         0.17992976\n",
            "  0.23285574 0.32871681 0.34944244 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute the Jaccard similarity matrix\n",
        "def jaccard_similarity_matrix(sentences):\n",
        "    \"\"\"Computes the Jaccard similarity matrix for a set of sentences.\"\"\"\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    # Initialize an empty matrix to store the similarities\n",
        "    similarity_matrix = cp.zeros((num_sentences, num_sentences))\n",
        "\n",
        "    # Compute Jaccard similarity for each pair of sentences\n",
        "    for i in range(num_sentences):\n",
        "        for j in range(i, num_sentences):  # Only compute upper triangle (since similarity is symmetric)\n",
        "            set_i = set(sentences[i])  # Convert sentence i to a set of words\n",
        "            set_j = set(sentences[j])  # Convert sentence j to a set of words\n",
        "\n",
        "            intersection = len(set_i & set_j)  # Intersection: common words\n",
        "            union = len(set_i | set_j)  # Union: all unique words\n",
        "\n",
        "            # Compute Jaccard similarity\n",
        "            similarity = intersection / union if union != 0 else 0\n",
        "            similarity_matrix[i, j] = similarity\n",
        "            similarity_matrix[j, i] = similarity  # Symmetric matrix\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "# Function to compute similarity for articles\n",
        "def compute_similarity_for_articles_jaccard(df, text_column='processed_text'):\n",
        "    \"\"\"Compute Jaccard similarity matrices for each article in a DataFrame.\"\"\"\n",
        "    text_series = df[text_column].to_pandas()  # Get text data from DataFrame\n",
        "    similarity_matrices = []\n",
        "\n",
        "    # Iterate over each article's processed text\n",
        "    for index, processed_text in tqdm(text_series.items(), total=len(text_series), desc=\"Processing Articles\"):\n",
        "        # Convert sentences to sets of words\n",
        "        processed_sentences = [set(sentence) for sentence in processed_text]\n",
        "\n",
        "        # Compute the Jaccard similarity matrix\n",
        "        similarity_matrix = jaccard_similarity_matrix(processed_sentences)\n",
        "\n",
        "        # Convert to CPU (NumPy) array for easier handling if necessary\n",
        "        similarity_matrix_cpu = cp.asnumpy(similarity_matrix)\n",
        "\n",
        "        # Store the similarity matrix for the article\n",
        "        similarity_matrices.append(similarity_matrix_cpu)\n",
        "\n",
        "    return similarity_matrices\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have a DataFrame `processed_df` with a 'processed_text' column\n",
        "final_similarity_matrices_jaccard = compute_similarity_for_articles_jaccard(processed_df, text_column='processed_text')\n",
        "\n",
        "# Example output for the first article\n",
        "print(\"Final Jaccard Similarity Matrix for the First Article:\\n\", final_similarity_matrices_jaccard[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQpj4zjUIKUl",
        "outputId": "5dc2c5fb-c64d-47b0-a81a-46ae8d14da47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Articles: 100%|██████████| 11490/11490 [03:27<00:00, 55.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Jaccard Similarity Matrix for the First Article:\n",
            " [[1.         0.05555556 0.         0.0625     0.03703704 0.\n",
            "  0.         0.1        0.05882353 0.05882353 0.05       0.10526316\n",
            "  0.07142857 0.         0.04761905 0.05      ]\n",
            " [0.05555556 1.         0.05       0.1        0.03125    0.\n",
            "  0.11111111 0.03846154 0.04545455 0.         0.08333333 0.04\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.05       1.         0.46153846 0.07142857 0.10526316\n",
            "  0.         0.04347826 0.         0.         0.         0.\n",
            "  0.         0.         0.04347826 0.        ]\n",
            " [0.0625     0.1        0.46153846 1.         0.06896552 0.04761905\n",
            "  0.         0.08695652 0.05       0.         0.04347826 0.04347826\n",
            "  0.         0.         0.04166667 0.        ]\n",
            " [0.03703704 0.03125    0.07142857 0.06896552 1.         0.17857143\n",
            "  0.         0.05882353 0.06666667 0.         0.06060606 0.06060606\n",
            "  0.         0.         0.02857143 0.        ]\n",
            " [0.         0.         0.10526316 0.04761905 0.17857143 1.\n",
            "  0.         0.03846154 0.         0.         0.04       0.\n",
            "  0.         0.         0.03846154 0.        ]\n",
            " [0.         0.11111111 0.         0.         0.         0.\n",
            "  1.         0.         0.05555556 0.         0.04761905 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.1        0.03846154 0.04347826 0.08695652 0.05882353 0.03846154\n",
            "  0.         1.         0.04       0.04       0.03571429 0.07407407\n",
            "  0.04545455 0.         0.07142857 0.03571429]\n",
            " [0.05882353 0.04545455 0.         0.05       0.06666667 0.\n",
            "  0.05555556 0.04       1.         0.1        0.13636364 0.5625\n",
            "  0.05555556 0.30769231 0.08333333 0.13636364]\n",
            " [0.05882353 0.         0.         0.         0.         0.\n",
            "  0.         0.04       0.1        1.         0.         0.13636364\n",
            "  0.05555556 0.13333333 0.44444444 0.13636364]\n",
            " [0.05       0.08333333 0.         0.04347826 0.06060606 0.04\n",
            "  0.04761905 0.03571429 0.13636364 0.         1.         0.12\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.10526316 0.04       0.         0.04347826 0.06060606 0.\n",
            "  0.         0.07407407 0.5625     0.13636364 0.12       1.\n",
            "  0.04761905 0.17647059 0.11538462 0.12      ]\n",
            " [0.07142857 0.         0.         0.         0.         0.\n",
            "  0.         0.04545455 0.05555556 0.05555556 0.         0.04761905\n",
            "  1.         0.07692308 0.04545455 0.1       ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.30769231 0.13333333 0.         0.17647059\n",
            "  0.07692308 1.         0.10526316 0.17647059]\n",
            " [0.04761905 0.         0.04347826 0.04166667 0.02857143 0.03846154\n",
            "  0.         0.07142857 0.08333333 0.44444444 0.         0.11538462\n",
            "  0.04545455 0.10526316 1.         0.16      ]\n",
            " [0.05       0.         0.         0.         0.         0.\n",
            "  0.         0.03571429 0.13636364 0.13636364 0.         0.12\n",
            "  0.1        0.17647059 0.16       1.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro8_S0vQa7Z0",
        "outputId": "f0024935-15cd-4a49-8b64-47b8172142fa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'article', 'highlights', 'processed_text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to Compute TextRank Scores Using Precomputed Similarity Matrices\n",
        "def compute_text_rank_from_similarity(similarity_matrix, damping_factor=0.85, max_iterations=100, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Compute TextRank scores using a precomputed similarity matrix.\n",
        "    :param similarity_matrix: Cosine similarity matrix for sentences.\n",
        "    :param damping_factor: Damping factor for the TextRank algorithm.\n",
        "    :param max_iterations: Maximum number of iterations for the algorithm.\n",
        "    :param tol: Convergence tolerance.\n",
        "    :return: TextRank scores for sentences.\n",
        "    \"\"\"\n",
        "    n = similarity_matrix.shape[0]\n",
        "    scores = cp.ones(n) / n  # Initialize scores uniformly\n",
        "    transition_matrix = similarity_matrix / (similarity_matrix.sum(axis=1, keepdims=True) + 1e-10)  # Normalize rows\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        new_scores = (1 - damping_factor) / n + damping_factor * cp.dot(transition_matrix.T, scores)\n",
        "        if cp.linalg.norm(new_scores - scores, ord=1) < tol:\n",
        "            break\n",
        "        scores = new_scores\n",
        "\n",
        "    return cp.asnumpy(scores)  # Convert to NumPy for easy handling\n",
        "\n",
        "\n",
        "# Function to Extract Top N Sentences Using Original Text\n",
        "def get_top_sentences_with_original_text(processed_text, original_sentences, scores, top_n=5):\n",
        "    \"\"\"\n",
        "    Get the top N sentences using TextRank scores in chronological order.\n",
        "    :param processed_text: List of preprocessed sentences (tokens).\n",
        "    :param original_sentences: List of original sentences.\n",
        "    :param scores: TextRank scores for the sentences.\n",
        "    :param top_n: Number of top sentences to select.\n",
        "    :return: List of top N original sentences.\n",
        "    \"\"\"\n",
        "    sentences_with_scores = [(idx, scores[idx]) for idx in range(len(scores))]\n",
        "\n",
        "    # Get the top N indices by score\n",
        "    top_indices = sorted(sentences_with_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    # Sort top indices in chronological order\n",
        "    top_indices_sorted = sorted(top_indices, key=lambda x: x[0])\n",
        "\n",
        "    # Retrieve original sentences using indices\n",
        "    top_sentences = [original_sentences[idx] for idx, _ in top_indices_sorted]\n",
        "    return top_sentences\n",
        "\n",
        "\n",
        "# Main Function to Rank Top Sentences Using Original Text and Precomputed Similarity Matrices\n",
        "def rank_top_sentences_with_original_text(df, similarity_matrices, text_column='processed_text', original_text_column='original_text', top_n=5):\n",
        "    \"\"\"\n",
        "    Rank top sentences for all articles using TextRank scores and precomputed similarity matrices.\n",
        "    :param df: DataFrame containing the articles.\n",
        "    :param similarity_matrices: List of similarity matrices for all articles.\n",
        "    :param text_column: Column containing preprocessed text (tokens).\n",
        "    :param original_text_column: Column containing original text (sentences).\n",
        "    :param top_n: Number of top sentences to select.\n",
        "    :return: List of lists of top sentences for each article.\n",
        "    \"\"\"\n",
        "    # First, convert the DataFrame to Pandas for sentence tokenization\n",
        "    df_cpu = df.to_pandas()\n",
        "\n",
        "    # Apply sentence tokenization to 'article' column (on CPU)\n",
        "    df_cpu['original_text'] = df_cpu['article'].apply(\n",
        "        lambda x: nltk.sent_tokenize(x)  # Split full article into sentences\n",
        "    )\n",
        "\n",
        "    # Convert back to cuDF for further processing (optional)\n",
        "    df_gpu = cudf.from_pandas(df_cpu)\n",
        "\n",
        "    text_series = df_gpu[text_column].to_pandas()  # Extract processed text\n",
        "    original_text_series = df_gpu[original_text_column].to_pandas()  # Extract original text\n",
        "    all_top_sentences = []\n",
        "\n",
        "    for index, (processed_text, original_sentences, similarity_matrix) in tqdm(\n",
        "        enumerate(zip(text_series, original_text_series, similarity_matrices)),\n",
        "        total=len(similarity_matrices),\n",
        "        desc=\"Ranking Sentences\"\n",
        "    ):\n",
        "        # Compute TextRank scores from the precomputed similarity matrix\n",
        "        text_rank_scores = compute_text_rank_from_similarity(cp.array(similarity_matrix))\n",
        "\n",
        "        # Get the top N sentences using original text\n",
        "        top_sentences = get_top_sentences_with_original_text(\n",
        "            processed_text, original_sentences, text_rank_scores, top_n=top_n\n",
        "        )\n",
        "\n",
        "        # Store the top sentences\n",
        "        all_top_sentences.append(top_sentences)\n",
        "\n",
        "    return all_top_sentences\n",
        "\n",
        "\n",
        "# Compute Top 5 Sentences Using Precomputed Similarity Matrices and Original Sentences\n",
        "top_sentences_per_article = rank_top_sentences_with_original_text(\n",
        "    processed_df, final_similarity_matrices, text_column='processed_text', original_text_column='original_text', top_n=5\n",
        ")\n",
        "\n",
        "# Example Output for the First Article\n",
        "print(\"Top 5 Sentences in Chronological Order for the First Article:\")\n",
        "for rank, sentence in enumerate(top_sentences_per_article[0], start=1):\n",
        "    print(f\"{rank}: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMv5ExsiKOih",
        "outputId": "067b11f6-eb7e-4dc9-d148-39637469acae"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ranking Sentences: 100%|██████████| 11490/11490 [04:19<00:00, 44.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Sentences in Chronological Order for the First Article:\n",
            "1: Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased .\n",
            "2: Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches .\n",
            "3: But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.\n",
            "4: While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches.\n",
            "5: British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}